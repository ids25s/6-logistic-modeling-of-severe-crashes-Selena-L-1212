---
title: "Homework Assignment 6"
author: "Yuxiao Lin"
toc: true
number-sections: true
highlight-style: pygments
format: 
  html: 
    code-fold: true
    html-math-method: katex
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
##  docx: Never, unless to accommodate a collaborator
---

## NYC Crash severity modeling
Using the cleaned NYC crash data, merged with zipcode level information, predict severe of a crash.
```{python}
import pandas as pd
import numpy as np

# Load the cleaned crash and zip code data
crash_data = pd.read_feather("/Users/yuxiaolin/Downloads/nyccrashes_cleaned.feather")
zip_data = pd.read_feather("/Users/yuxiaolin/Downloads/nyc_zip_areas.feather")

# Drop NaN values in 'zip_code'
crash_data = crash_data.dropna(subset=['zip_code'])

# check the dtypes of zipcode
print("zip_code's type:", crash_data['zip_code'].dtype)
print("modzcta's type:", zip_data['modzcta'].dtype)

# Convert 'zip_code' and 'modzcta' to strings
crash_data['zipcode'] = crash_data['zip_code'].astype(str).str.zfill(5)
zip_data['zipcode'] = zip_data['modzcta'].astype(str).str.zfill(5) 

# Check that the conversion was correct
print("zipcode's type:", crash_data['zipcode'].dtype)
print("zipcode's type:", zip_data['zipcode'].dtype)

# load acs data, and convert 'zip_code' to sting
acs_data = pd.read_feather("/Users/yuxiaolin/Downloads/acs2023.feather")
acs_data.columns = acs_data.columns.str.lower().str.replace(' ', '_')
acs_data['zipcode'] = acs_data['zip_code'].astype(str).str.zfill(5) 
print("acs_code's type:", acs_data['zipcode'].dtype)
```



```{python}
# convert crash_data to numeric, then change back to string, in order to delete float
crash_data['zipcode'] = pd.to_numeric(crash_data['zipcode'], errors='coerce')
crash_data['zipcode'] = crash_data['zipcode'].fillna(0).astype(int).astype(str).str.zfill(5)

# check two dataset whether have same output type
print(crash_data['zipcode'].unique()[:10])
print(zip_data['zipcode'].unique()[:10])
```



```{python}
# create a new column 'hour'
crash_data['crash_datetime'] = pd.to_datetime(crash_data['crash_datetime'])
crash_data['hour'] = crash_data['crash_datetime'].dt.hour

# create a new variable "severe"
crash_data["severe"] = ((crash_data["number_of_persons_injured"] + \
    crash_data["number_of_persons_killed"]) >= 1).astype(int)
```

```{python}
# merge datasets on zipcode
merged_data = crash_data.merge(zip_data, on='zipcode', how='left')
merged_data = merged_data.merge(acs_data, on='zipcode', how='left')
```





### 1. Set random seed to 1234. Randomly select 20% of the crashes as testing data and leave the rest 80% as training data.

```{python}
np.random.seed(1234)

merged_data.head(20)
print(merged_data.columns)
```


```{python}
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, confusion_matrix,
    f1_score, roc_curve, auc
)
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification

#features
X = merged_data[["hour", "zip_code_x", "white_population", "black_population", "asian_population", "labor_force", "unemployed"]]
y = merged_data['severe']

# delete the rows that including missing value
X = X.dropna()
y = y.loc[X_clean.index] 

# randomly select testing data 20%, training data 80%
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
```


